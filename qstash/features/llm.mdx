---
title: "LLM"
---

QStash has built-in support for calling LLM APIs. This allows you to take advantage of QStash features such as retries, callbacks, and batching while using LLM APIs. 

QStash is especially useful for LLM processing because LLM response times are often highly variable. When accessing LLM APIs from serverless runtimes, invocation timeouts are a common issue. QStash offers an HTTP timeout of 2 hours, which is sufficient for most LLM use cases. By using callbacks and the workflows, you can easily manage the asynchronous nature of LLM APIs.

## QStash LLM API

You can publish (or enqueue) single LLM request or batch LLM requests using all existing QStash features natively. To do this, specify the destination `api` as `llm` with a valid provider. The body of the published or enqueued message should contain a valid chat completion request. For these integrations, you must specify the `Upstash-Callback` header so that you can process the response asynchronously. Note that streaming chat completions cannot be used with them. Use [the chat API](#chat-api) for streaming completions.

All the examples below can also be used with **OpenAI-compatible LLM providers**, in addition to Upstash-hosted models. Soon, we will add LLM providers other than OpenAI and Upstash.

### Publishing a Chat Completion Request

<CodeGroup>
```js JavaScript
import { Client, upstash } from "@upstash/qstash";

const client = new Client({
    token: "<QSTASH_TOKEN>",
});

const result = await client.publishJSON({
    api: { name: "llm", provider: openai({ token: "_OPEN_AI_TOKEN_"}) },
    body: {
        model: "gpt-3.5-turbo",
        messages: [
            {
            role: "user",
            content: "Write a hello world program in Rust.",
            },
        ],
    },
    callback: "https://abc.requestcatcher.com/",
});

console.log(result);
```

```python Python
from qstash import QStash
from qstash.chat import upstash

q = QStash("<QSTASH_TOKEN>")

result = q.message.publish_json(
    api={"name": "llm", "provider": openai("<OPENAI_API_KEY>")},
    body={
        "model": "gpt-3.5-turbo",
        "messages": [
            {
                "role": "user",
                "content": "Write a hello world program in Rust.",
            }
        ],
    },
    callback="https://abc.requestcatcher.com/",
)

print(result)
```

```shell curl
curl "https://qstash.upstash.io/v2/publish/api/llm" \
    -X POST \
    -H "Authorization: Bearer QSTASH_TOKEN" \
    -H "Content-Type: application/json" \
    -H "Upstash-Callback: https://abc.requestcatcher.com" \
    -d '{
        "model": "meta-llama/Meta-Llama-3-8B-Instruct",
        "messages": [
            {
                "role": "user",
                "content": "Write a hello world program in Rust."
            }
        ]
    }'
```
</CodeGroup>

### Enqueueing a Chat Completion Request

<CodeGroup>
```js JavaScript
import { Client, upstash } from "@upstash/qstash";

const client = new Client({
    token: "<QSTASH_TOKEN>",
});

const result = await client.queue({ queueName: "queue-name" }).enqueueJSON({
    api: { name: "llm", provider: openai({ token: "_OPEN_AI_TOKEN_"}) },
    body: {
        model: "gpt-3.5-turbo",
        messages: [
            {
                role: "user",
                content: "Write a hello world program in Rust.",
            },
        ],
    },
    callback: "https://abc.requestcatcher.com",
});

console.log(result);
```

```python Python
from qstash import QStash
from qstash.chat import upstash

q = QStash("<QSTASH_TOKEN>")

result = q.message.enqueue_json(
    queue="queue-name",
    api={"name": "llm", "provider": openai("<OPENAI_API_KEY>")},
    body={
        "model": "gpt-3.5-turbo",
        "messages": [
            {
                "role": "user",
                "content": "Write a hello world program in Rust.",
            }
        ],
    },
    callback="https://abc.requestcatcher.com",
)

print(result)
```

```shell curl
curl "https://qstash.upstash.io/v2/enqueue/queue-name/api/llm" \
    -X POST \
    -H "Authorization: Bearer QSTASH_TOKEN" \
    -H "Content-Type: application/json" \
    -H "Upstash-Callback: https://abc.requestcatcher.com" \
    -d '{
        "model": "meta-llama/Meta-Llama-3-8B-Instruct",
        "messages": [
            {
                "role": "user",
                "content": "What is the first prime number?"
            }
        ]
    }'
```
</CodeGroup>

### Sending Chat Completion Requests in Batches

<CodeGroup>
```js JavaScript
import { Client, upstash } from "@upstash/qstash";

const client = new Client({
    token: "<QSTASH_TOKEN>",
});

const result = await client.batchJSON([
    {
        api: { name: "llm", provider: openai({ token: "_OPEN_AI_TOKEN_" }) },
        body: { ... },
        callback: "https://abc.requestcatcher.com",
    },
    ...
]);

console.log(result);
```

```python Python
from qstash import QStash
from qstash.chat import upstash

q = QStash("<QSTASH_TOKEN>")

result = q.message.batch_json(
    [
        {
            "api":{"name": "llm", "provider": openai("<OPENAI_API_KEY>")},
            "body": {...},
            "callback": "https://abc.requestcatcher.com",
        },
        ...
    ]
)

print(result)
```

```shell curl
curl "https://qstash.upstash.io/v2/batch" \
    -X POST \
    -H "Authorization: Bearer QSTASH_TOKEN" \
    -H "Content-Type: application/json" \
    -d '[
        {
            "destination": "api/llm",
            "body": {...},
            "callback": "https://abc.requestcatcher.com"
        },
        ...
    ]'
```
</CodeGroup>

### Retrying After Rate Limit Resets

When the rate limits are exceeded, QStash automatically schedules the retry of
publish or enqueue of chat completion tasks depending on the reset time
of the rate limits. That helps with not doing retries prematurely
when it is definitely going to fail due to exceeding rate limits.


## Upstash Hosted Models

You can use Upstash-hosted models for LLM completions. Upstash offers a hosted LLM service compatible with OpenAI, currently supporting the following models:

-  `meta-llama/Meta-Llama-3-8B-Instruct`
-  `mistralai/Mistral-7B-Instruct-v0.2`

Based on adoption, we plan to add more models in the future. Upstash-hosted models are priced under QStash at $0.3 per 1M tokens. Use the `upstash` provider to access these models.
                  

## Chat API
While the use of `publish` is encouraged, a synchronous `chat` API is also available for immediate completions. Chat API can be used with both Upstash and OpenAI providers.

<CodeGroup>
```js JavaScript
import { Client, upstash } from "@upstash/qstash";

const client = new Client({
    token: "<QSTASH_TOKEN>",
});

const completion = await client.chat().create({
    provider: upstash(),
    model: "meta-llama/Meta-Llama-3-8B-Instruct",
    messages: [
        {
            role: "user",
            content: "What is a large language model?",
        },
    ],
});

console.log(completion);
```

```python Python
from qstash import QStash
from qstash.chat import upstash

q = QStash("<QSTASH_TOKEN>")

completion = q.chat.create(
    provider=upstash(),
    model="meta-llama/Meta-Llama-3-8B-Instruct",
    messages=[
        {
            "role": "user",
            "content": "What is a large language model?",
        },
    ],
)

print(completion)
```

```shell curl
curl "https://qstash.upstash.io/llm/v1/chat/completions" \
    -X POST \
    -H "Authorization: Bearer QSTASH_TOKEN" \
    -H "Content-Type: application/json" \
    -d '{
        "model": "meta-llama/Meta-Llama-3-8B-Instruct",
        "messages": [
            {
                "role": "user",
                "content": "What is a large language model? Explain like I am five."
            }
        ]
    }'
```
</CodeGroup>

It is also possible to simulate a chat session by passing the history
of user and assistant messages to the request.

<CodeGroup>
```js JavaScript
import { Client, upstash } from "@upstash/qstash";

const client = new Client({
    token: "<QSTASH_TOKEN>",
});

const completion = await client.chat().create({
    provider: upstash(),
    model: "meta-llama/Meta-Llama-3-8B-Instruct",
    messages: [
        {
            role: "user",
            content: "What is a large language model?",
        },
        {
            role: "assistant",
            content: "A large language model is a type of artificial intelligence model that is trained on a massive dataset of text to generate human-like language outputs.",
        },
        {
            role: "user",
            content: "What are some use cases that can benefit from LLMs?",
        },
    ],
});

console.log(completion);
```

```python Python
from qstash import QStash
from qstash.chat import upstash

q = QStash("<QSTASH_TOKEN>")

completion = q.chat.create(
    provider=upstash(),
    model="meta-llama/Meta-Llama-3-8B-Instruct",
    messages=[
        {
            "role": "user",
            "content": "What is a large language model?",
        },
        {
            "role": "assistant",
            "content": "A large language model is a type of artificial intelligence model that is trained on a massive dataset of text to generate human-like language outputs.",
        },
        {
            "role": "user",
            "content": "What are some use cases that can benefit from LLMs?",
        },
    ],
)

print(completion)
```

```shell curl
curl "https://qstash.upstash.io/llm/v1/chat/completions" \
    -X POST \
    -H "Authorization: Bearer QSTASH_TOKEN" \
    -H "Content-Type: application/json" \
    -d '{
        "model": "meta-llama/Meta-Llama-3-8B-Instruct",
        "messages": [
            {
                "role": "user",
                "content": "What is a large language model?"
            },
            {
                "role": "assistant",
                "content": "A large language model is a type of artificial intelligence model that is trained on a massive dataset of text to generate human-like language outputs."
            },
            {
                "role": "user",
                "content": "What are some use cases that can benefit from LLMs?"
            }
        ]
    }'
```
</CodeGroup>

Chat completion responses can also be delivered in chunk streams.

<CodeGroup>
```js JavaScript
import { Client, upstash } from "@upstash/qstash";

const client = new Client({
    token: "<QSTASH_TOKEN>",
});

const completion = await client.chat().create({
    provider: upstash(),
    model: "meta-llama/Meta-Llama-3-8B-Instruct",
    messages: [
        {
            role: "user",
            content: "What is a large language model?",
        },
    ],
    stream: true,
});

for await (const chunk of completion) {
    console.log(chunk);
}
```

```python Python
from qstash import QStash
from qstash.chat import upstash

q = QStash("<QSTASH_TOKEN>")

completion = q.chat.create(
    provider=upstash(),
    model="meta-llama/Meta-Llama-3-8B-Instruct",
    messages=[
        {
            "role": "user",
            "content": "What is a large language model?",
        },
    ],
    stream=True,
)

for chunk in completion:
    print(chunk)
```

```shell curl
curl "https://qstash.upstash.io/llm/v1/chat/completions" \
    -X POST \
    -N \
    -H "Authorization: Bearer QSTASH_TOKEN" \
    -H "Content-Type: application/json" \
    -d '{
        "model": "meta-llama/Meta-Llama-3-8B-Instruct",
        "messages": [
            {
                "role": "user",
                "content": "What is a large language model?"
            }
        ],
        "stream": true
    }'
```
</CodeGroup>

The code snippets above can also be run with other LLM providers
like OpenAI, just by specifying it as a provider and passing
the API key.


<CodeGroup>
```js JavaScript
import { Client, openai } from "@upstash/qstash";

const client = new Client({
    token: "<QSTASH_TOKEN>",
});

const completion = await client.chat().create({
    provider: openai({ token: "<OPENAI_API_KEY>" }),
    model: "gpt-3.5-turbo",
    messages: [
        {
            role: "user",
            content: "What is a large language model?",
        },
    ],
});

console.log(completion);
```

```python Python
from qstash import QStash
from qstash.chat import openai

q = QStash("<QSTASH_TOKEN>")

completion = q.chat.create(
    provider=openai(token="OPENAI_API_KEY"),
    model="gpt-3.5-turbo",
    messages=[
        {
            "role": "user",
            "content": "What is a large language model?",
        },
    ],
)

print(completion)
```
</CodeGroup>

The chat completions endpoint is also compatible with the OpenAI REST API,
so you can use wide variety of tools and libraries, including the official
OpenAI Python client with it.

```python
from openai import OpenAI

client = OpenAI(
    base_url="https://qstash.upstash.io/llm/v1",
    api_key="<QSTASH_TOKEN>",
)

completion = client.chat.completions.create(
    messages=[
        {
            "role": "user",
            "content": "Test message from the OpenAI client",
        }
    ],
    model="meta-llama/Meta-Llama-3-8B-Instruct",
)

print(completion)
```

## Analytics via Helicone

Helicone is a powerful observability platform that provides valuable insights into your LLM usage. Integrating Helicone with QStash is straightforward.

To enable Helicone observability in QStash, you simply need to pass your Helicone API key when initializing your model. Here's how to do it for both custom models and OpenAI:


```ts
import { Client, custom } from "@upstash/qstash";

const client = new Client({
  token: "<QSTASH_TOKEN>",
});

await client.publishJSON({
  api: {
    name: "llm",
    provider: custom({
      token: "XXX",
      baseUrl: "https://api.together.xyz",
    }),
    analytics: { name: "helicone", token: process.env.HELICONE_API_KEY! },
  },
  body: {
    model: "meta-llama/Llama-3-8b-chat-hf",
    messages: [
      {
        role: "user",
        content: "hello",
      },
    ],
  },
  callback: "https://oz.requestcatcher.com/",
});
```

